{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openai\n!pip install --upgrade openaiimport numpy as np \nimport openai\nimport json\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\napi_key = \"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\"\nopenai.api_key = api_key","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-05-30T16:38:12.624141Z","iopub.execute_input":"2023-05-30T16:38:12.624395Z","iopub.status.idle":"2023-05-30T16:38:25.904760Z","shell.execute_reply.started":"2023-05-30T16:38:12.624373Z","shell.execute_reply":"2023-05-30T16:38:25.903342Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting openai\n  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.28.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.5.7)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\nInstalling collected packages: openai\nSuccessfully installed openai-0.27.7\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement openaiimport (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for openaiimport\u001b[0m\u001b[31m\n\u001b[0m/kaggle/input/taiwan-112/112_1301.csv\n/kaggle/input/taiwan-112/112_3302.csv\n/kaggle/input/taiwan-112/112_4302.csv\n/kaggle/input/taiwan-112/112_2301.csv\n/kaggle/input/taiwan-112/112_2302.csv\n/kaggle/input/taiwan-112/112_1302.csv\n/kaggle/input/taiwan-112-translated/112_3302_model4.csv\n/kaggle/input/taiwan-112-translated/112_2302_model3.5.csv\n/kaggle/input/taiwan-112-translated/112_3302_model3.5.csv\n/kaggle/input/taiwan-112-translated/112_2301_model3.5.csv\n/kaggle/input/taiwan-112-translated/112_4302_model4.csv\n/kaggle/input/taiwan-112-translated/112_1302_model3.5.csv\n/kaggle/input/taiwan-112-translated/112_2302_model4.csv\n/kaggle/input/taiwan-112-translated/112_4302_model3.5.csv\n/kaggle/input/taiwan-112-translated/112_1302_model4.csv\n/kaggle/input/taiwan-112-translated/112_1301_model3.5.csv\n/kaggle/input/taiwan-112-translated/112_1301_model4.csv\n/kaggle/input/taiwan-112-translated/112_2301_model4.csv\n/kaggle/input/taiwan-112-answered/112_1301_4_p2_en.csv\n/kaggle/input/taiwan-112-answered/112_1301_4_sp_en.csv\n/kaggle/input/taiwan-112-answered/112_1301_4_np_tw.csv\n/kaggle/input/taiwan-112-answered/112_1301_4_np_en.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"!rm -rf ./*","metadata":{"execution":{"iopub.status.busy":"2023-05-30T17:34:17.795234Z","iopub.execute_input":"2023-05-30T17:34:17.795614Z","iopub.status.idle":"2023-05-30T17:34:18.062951Z","shell.execute_reply.started":"2023-05-30T17:34:17.795588Z","shell.execute_reply":"2023-05-30T17:34:18.061779Z"}}},{"cell_type":"code","source":"def get_q(year, code, question):\n    file_path = f'/kaggle/input/taiwan-112-translated/{year}_{code}_model4.csv'\n    df = pd.read_csv(file_path)\n    if question > len(df) or question < 1:\n        raise IndexError(f\"Question index out of range. Please enter a value between 1 and {len(df)}.\")\n    nth_question = df.iloc[question - 1, 7]  # target located at the 8th column\n    return nth_question\n\ndef get_a(year, code, question):\n    file_path = f'/kaggle/input/taiwan-112-answered/{year}_{code}_4_p2_en.csv'\n    df = pd.read_csv(file_path)\n    if question > len(df) or question < 1:\n        raise IndexError(f\"Question index out of range. Please enter a value between 1 and {len(df)}.\")\n    nth_question = df.iloc[question - 1, 7]  # target located at the 8th column\n    return nth_question\n\ndef gpt_ans(year, code, question, model):\n    if model == 4:\n        model_name = \"gpt-4\"\n    elif model == 3.5:\n        model_name = \"gpt-3.5-turbo\"\n    else:\n        raise ValueError(\"Invalid model value. Must be either 4 or 3.5\")\n\n    user_prompt = get_q(year, code, question)\n    assistance_prompt = get_a(year, code, question)\n\n    response = openai.ChatCompletion.create(\n        model=model_name,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a medical doctor.\"},\n            {\"role\": \"user\", \"content\": user_prompt},\n            {\"role\": \"assistant\", \"content\": assistance_prompt},\n            {\"role\": \"user\", \"content\": \"Please carefully review the question and your previous response for accuracy in accordance with medical knowledge, logical consistency, and proper interpretation. You can stand by your original answer or change the answer if you find any inconsistencies based on your training data. Update the consistency score on a scale of 1 to 10. Avoid repeating existing information from the question or prior response to save tokens.\"},\n        ],\n        max_tokens=2048,\n        temperature=0,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0\n    )\n    return response\n\ndef gpt_ans_loop(year, code, q_start, q_fin, model):\n    data = {\n        'year': [],\n        'code': [],\n        'question_no': [],\n        'id': [],\n        'created': [],\n        'model': [],\n        'object': [],\n        'message_content': [],\n        'message_role': [],\n        'finish_reason': [],\n        'index': [],\n        'completion_tokens': [],\n        'prompt_tokens': [],\n        'total_tokens': [],\n    }\n\n    last_answered_q = q_start - 1\n    try:\n        for question in range(q_start, q_fin+1):\n            response = gpt_ans(year, code, question, model)\n\n            data['year'].append(year)\n            data['code'].append(code)\n            data['question_no'].append(question)\n            data['id'].append(response['id'])\n            data['created'].append(response['created'])\n            data['model'].append(response['model'])\n            data['object'].append(response['object'])\n            data['message_content'].append(response['choices'][0]['message']['content'])\n            data['message_role'].append(response['choices'][0]['message']['role'])\n            data['finish_reason'].append(response['choices'][0]['finish_reason'])\n            data['index'].append(response['choices'][0]['index'])\n            data['completion_tokens'].append(response['usage']['completion_tokens'])\n            data['prompt_tokens'].append(response['usage']['prompt_tokens'])\n            data['total_tokens'].append(response['usage']['total_tokens'])\n\n            print(f'Question {question} has been answered.')\n            last_answered_q = question\n    except IndexError:\n        print(\"Question number out of range. Exporting the data up to the last valid question.\")\n    finally:\n        df = pd.DataFrame(data)\n        filename = f'{year}_{code}_{q_start}_{last_answered_q}_{model}_ec.csv'\n        df.to_csv(filename, index=False)\n        print(f'Questions {q_start} to {last_answered_q} have been answered and saved as {filename}.')\n        estimate_cost(df, model)\n\n        return df\n\n\ndef estimate_cost(df, model):\n    if model == 4:\n        cost_prompt = df['prompt_tokens'].sum() * 0.03 / 1000\n        cost_completion = df['completion_tokens'].sum() * 0.06 / 1000\n        total_cost = cost_prompt + cost_completion\n    elif model == 3.5:\n        total_cost = df['total_tokens'].sum() * 0.002 / 1000\n\n    print(f'Estimated API call cost: ${total_cost:.6f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:47:40.235560Z","iopub.execute_input":"2023-05-30T18:47:40.235978Z","iopub.status.idle":"2023-05-30T18:47:40.255893Z","shell.execute_reply.started":"2023-05-30T18:47:40.235947Z","shell.execute_reply":"2023-05-30T18:47:40.254834Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"gpt_ans_loop(112, 1301, 37, 100, 4)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-05-30T19:01:57.631940Z","iopub.execute_input":"2023-05-30T19:01:57.632606Z","iopub.status.idle":"2023-05-30T19:26:12.938616Z","shell.execute_reply.started":"2023-05-30T19:01:57.632548Z","shell.execute_reply":"2023-05-30T19:26:12.937626Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Question 37 has been answered.\nQuestion 38 has been answered.\nQuestion 39 has been answered.\nQuestion 40 has been answered.\nQuestion 41 has been answered.\nQuestion 42 has been answered.\nQuestion 43 has been answered.\nQuestion 44 has been answered.\nQuestion 45 has been answered.\nQuestion 46 has been answered.\nQuestion 47 has been answered.\nQuestion 48 has been answered.\nQuestion 49 has been answered.\nQuestion 50 has been answered.\nQuestion 51 has been answered.\nQuestion 52 has been answered.\nQuestion 53 has been answered.\nQuestion 54 has been answered.\nQuestion 55 has been answered.\nQuestion 56 has been answered.\nQuestion 57 has been answered.\nQuestion 58 has been answered.\nQuestion 59 has been answered.\nQuestion 60 has been answered.\nQuestion 61 has been answered.\nQuestion 62 has been answered.\nQuestion 63 has been answered.\nQuestion 64 has been answered.\nQuestion 65 has been answered.\nQuestion 66 has been answered.\nQuestion 67 has been answered.\nQuestion 68 has been answered.\nQuestion 69 has been answered.\nQuestion 70 has been answered.\nQuestion 71 has been answered.\nQuestion 72 has been answered.\nQuestion 73 has been answered.\nQuestion 74 has been answered.\nQuestion 75 has been answered.\nQuestion 76 has been answered.\nQuestion 77 has been answered.\nQuestion 78 has been answered.\nQuestion 79 has been answered.\nQuestion 80 has been answered.\nQuestion 81 has been answered.\nQuestion 82 has been answered.\nQuestion 83 has been answered.\nQuestion 84 has been answered.\nQuestion 85 has been answered.\nQuestion 86 has been answered.\nQuestion 87 has been answered.\nQuestion 88 has been answered.\nQuestion 89 has been answered.\nQuestion 90 has been answered.\nQuestion 91 has been answered.\nQuestion 92 has been answered.\nQuestion 93 has been answered.\nQuestion 94 has been answered.\nQuestion 95 has been answered.\nQuestion 96 has been answered.\nQuestion 97 has been answered.\nQuestion 98 has been answered.\nQuestion 99 has been answered.\nQuestion 100 has been answered.\nQuestions 37 to 100 have been answered and saved as 112_1301_37_100_4_ec.csv.\nEstimated API call cost: $1.171440\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"    year  code  question_no                                      id  \\\n0    112  1301           37  chatcmpl-7LytRZEUbtNWNkuQh3r8r4JRqkKlV   \n1    112  1301           38  chatcmpl-7LytiL1FnRHnwVkIomoijJLLvIqWb   \n2    112  1301           39  chatcmpl-7Lyu9XSj4ncd3rNJkAKHH9biK0ALH   \n3    112  1301           40  chatcmpl-7LyuMPEBsMspCZbDkpDGTe58lJQ8D   \n4    112  1301           41  chatcmpl-7LyueAgkLnUbCe9HiEjT69RAaPmUC   \n..   ...   ...          ...                                     ...   \n59   112  1301           96  chatcmpl-7LzFnowH9ZGU5OH8GiBg9hq9NciLJ   \n60   112  1301           97  chatcmpl-7LzFzoDIrhFVdMP8qy6yClWUDbMX9   \n61   112  1301           98  chatcmpl-7LzGHI73TUciIbXf81bCDB88u03IE   \n62   112  1301           99  chatcmpl-7LzGTvFLavM92sZz4MmyrTGk9BMu1   \n63   112  1301          100  chatcmpl-7LzGhNpPSl1S7EcWsQGOUdM6AQULY   \n\n       created       model           object  \\\n0   1685473317  gpt-4-0314  chat.completion   \n1   1685473334  gpt-4-0314  chat.completion   \n2   1685473361  gpt-4-0314  chat.completion   \n3   1685473374  gpt-4-0314  chat.completion   \n4   1685473392  gpt-4-0314  chat.completion   \n..         ...         ...              ...   \n59  1685474703  gpt-4-0314  chat.completion   \n60  1685474715  gpt-4-0314  chat.completion   \n61  1685474733  gpt-4-0314  chat.completion   \n62  1685474745  gpt-4-0314  chat.completion   \n63  1685474759  gpt-4-0314  chat.completion   \n\n                                      message_content message_role  \\\n0   Upon careful review, I stand by my original an...    assistant   \n1   Upon careful review, I realize that I made an ...    assistant   \n2   Upon careful review, I stand by my original an...    assistant   \n3   Upon careful review, I stand by my original an...    assistant   \n4   Upon further review, I stand by my original an...    assistant   \n..                                                ...          ...   \n59  Upon careful review, I stand by my original an...    assistant   \n60  Upon careful review, I stand by my original an...    assistant   \n61  Upon further review, I stand by my original an...    assistant   \n62  Upon careful review, I stand by my original an...    assistant   \n63  Upon careful review, I stand by my original an...    assistant   \n\n   finish_reason  index  completion_tokens  prompt_tokens  total_tokens  \n0           stop      0                 66            407           473  \n1           stop      0                113            307           420  \n2           stop      0                 48            415           463  \n3           stop      0                 72            450           522  \n4           stop      0                 76            389           465  \n..           ...    ...                ...            ...           ...  \n59          stop      0                 54            364           418  \n60          stop      0                 69            364           433  \n61          stop      0                 50            328           378  \n62          stop      0                 61            430           491  \n63          stop      0                 51            354           405  \n\n[64 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>code</th>\n      <th>question_no</th>\n      <th>id</th>\n      <th>created</th>\n      <th>model</th>\n      <th>object</th>\n      <th>message_content</th>\n      <th>message_role</th>\n      <th>finish_reason</th>\n      <th>index</th>\n      <th>completion_tokens</th>\n      <th>prompt_tokens</th>\n      <th>total_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>37</td>\n      <td>chatcmpl-7LytRZEUbtNWNkuQh3r8r4JRqkKlV</td>\n      <td>1685473317</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon careful review, I stand by my original an...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>66</td>\n      <td>407</td>\n      <td>473</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>38</td>\n      <td>chatcmpl-7LytiL1FnRHnwVkIomoijJLLvIqWb</td>\n      <td>1685473334</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon careful review, I realize that I made an ...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>113</td>\n      <td>307</td>\n      <td>420</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>39</td>\n      <td>chatcmpl-7Lyu9XSj4ncd3rNJkAKHH9biK0ALH</td>\n      <td>1685473361</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon careful review, I stand by my original an...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>48</td>\n      <td>415</td>\n      <td>463</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>40</td>\n      <td>chatcmpl-7LyuMPEBsMspCZbDkpDGTe58lJQ8D</td>\n      <td>1685473374</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon careful review, I stand by my original an...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>72</td>\n      <td>450</td>\n      <td>522</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>41</td>\n      <td>chatcmpl-7LyueAgkLnUbCe9HiEjT69RAaPmUC</td>\n      <td>1685473392</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon further review, I stand by my original an...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>76</td>\n      <td>389</td>\n      <td>465</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>96</td>\n      <td>chatcmpl-7LzFnowH9ZGU5OH8GiBg9hq9NciLJ</td>\n      <td>1685474703</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon careful review, I stand by my original an...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>54</td>\n      <td>364</td>\n      <td>418</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>97</td>\n      <td>chatcmpl-7LzFzoDIrhFVdMP8qy6yClWUDbMX9</td>\n      <td>1685474715</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon careful review, I stand by my original an...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>69</td>\n      <td>364</td>\n      <td>433</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>98</td>\n      <td>chatcmpl-7LzGHI73TUciIbXf81bCDB88u03IE</td>\n      <td>1685474733</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon further review, I stand by my original an...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>50</td>\n      <td>328</td>\n      <td>378</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>99</td>\n      <td>chatcmpl-7LzGTvFLavM92sZz4MmyrTGk9BMu1</td>\n      <td>1685474745</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon careful review, I stand by my original an...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>61</td>\n      <td>430</td>\n      <td>491</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>112</td>\n      <td>1301</td>\n      <td>100</td>\n      <td>chatcmpl-7LzGhNpPSl1S7EcWsQGOUdM6AQULY</td>\n      <td>1685474759</td>\n      <td>gpt-4-0314</td>\n      <td>chat.completion</td>\n      <td>Upon careful review, I stand by my original an...</td>\n      <td>assistant</td>\n      <td>stop</td>\n      <td>0</td>\n      <td>51</td>\n      <td>354</td>\n      <td>405</td>\n    </tr>\n  </tbody>\n</table>\n<p>64 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"gpt_ans(112, 1301, 48, 4)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T17:28:38.401084Z","iopub.execute_input":"2023-05-30T17:28:38.401464Z","iopub.status.idle":"2023-05-30T17:29:31.792763Z","shell.execute_reply.started":"2023-05-30T17:28:38.401433Z","shell.execute_reply":"2023-05-30T17:29:31.791564Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<OpenAIObject chat.completion id=chatcmpl-7LxR8yO3YJMdtgN5K6iqbHehjR6Eb at 0x7a35817372e0> JSON: {\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"Upon further review, I realize that my previous response contained an error in the direction of nystagmus. I apologize for the confusion. The correct answer should be:\\n\\nB. Rightward\\n\\nWhen cold water is injected into the left ear, it causes the endolymphatic fluid in the horizontal semicircular canal to become more dense and sink, leading to a decrease in the firing rate of the vestibular nerve on that side. This creates an imbalance between the left and right vestibular systems, causing the eyes to move slowly towards the side with the lower firing rate (in this case, the left side). However, the brain will attempt to correct this imbalance by initiating a rapid eye movement in the opposite direction (rightward). This slow leftward movement followed by a rapid rightward movement is known as nystagmus.\\n\\nAfter reevaluating my response, I would rate the consistency of this revised answer as a 10 out of 10.\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1685467718,\n  \"id\": \"chatcmpl-7LxR8yO3YJMdtgN5K6iqbHehjR6Eb\",\n  \"model\": \"gpt-4-0314\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 193,\n    \"prompt_tokens\": 563,\n    \"total_tokens\": 756\n  }\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Code for GPT translation","metadata":{}},{"cell_type":"markdown","source":"def get_q(year, code, question):\n    file_path = f'/kaggle/input/taiwan-112/{year}_{code}.csv'\n    df = pd.read_csv(file_path)\n    if question > len(df) or question < 1:\n        raise IndexError(f\"Question index out of range. Please enter a value between 1 and {len(df)}.\")\n    nth_question = df.iloc[question - 1, -1]\n    return nth_question\n\ndef gpt_trans(year, code, question, model):\n    if model == 4:\n        model_name = \"gpt-4\"\n    elif model == 3.5:\n        model_name = \"gpt-3.5-turbo\"\n    else:\n        raise ValueError(\"Invalid model value. Must be either 4 or 3.5\")\n    response = openai.ChatCompletion.create(\n        model=model_name,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a professional medical translator.\"},\n            {\"role\": \"user\", \"content\": f'translate the following question from zh-tw to en-us in proper medical terminology: {get_q(year, code, question)}'}\n        ],\n        max_tokens=1024,\n        temperature=0,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0\n    )\n    return response\n\n\ndef gpt_trans_loop(year, code, q_start, q_fin, model):\n    data = {\n        'year': [],\n        'code': [],\n        'question_no': [],\n        'id': [],\n        'created': [],\n        'model': [],\n        'object': [],\n        'message_content': [],\n        'message_role': [],\n        'finish_reason': [],\n        'index': [],\n        'completion_tokens': [],\n        'prompt_tokens': [],\n        'total_tokens': [],\n    }\n\n    last_translated_q = q_start - 1\n    try:\n        for question in range(q_start, q_fin+1):\n            response = gpt_trans(year, code, question, model)\n\n            data['year'].append(year)\n            data['code'].append(code)\n            data['question_no'].append(question)\n            data['id'].append(response['id'])\n            data['created'].append(response['created'])\n            data['model'].append(response['model'])\n            data['object'].append(response['object'])\n            data['message_content'].append(response['choices'][0]['message']['content'])\n            data['message_role'].append(response['choices'][0]['message']['role'])\n            data['finish_reason'].append(response['choices'][0]['finish_reason'])\n            data['index'].append(response['choices'][0]['index'])\n            data['completion_tokens'].append(response['usage']['completion_tokens'])\n            data['prompt_tokens'].append(response['usage']['prompt_tokens'])\n            data['total_tokens'].append(response['usage']['total_tokens'])\n\n            print(f'Question {question} has been translated.')\n            last_translated_q = question\n    except IndexError:\n        print(\"Question number out of range. Exporting the data up to the last valid question.\")\n    finally:\n        df = pd.DataFrame(data)\n        filename = f'{year}_{code}_{q_start}_{last_translated_q}_model{model}.csv'\n        df.to_csv(filename, index=False)\n        print(f'Questions {q_start} to {last_translated_q} have been translated and saved as {filename}.')\n        estimate_cost(df, model)\n\n        return df\n\n\ndef estimate_cost(df, model):\n    if model == 4:\n        cost_prompt = df['prompt_tokens'].sum() * 0.03 / 1000\n        cost_completion = df['completion_tokens'].sum() * 0.06 / 1000\n        total_cost = cost_prompt + cost_completion\n    elif model == 3.5:\n        total_cost = df['total_tokens'].sum() * 0.002 / 1000\n\n    print(f'Estimated API call cost: ${total_cost:.6f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-29T16:59:05.027019Z","iopub.execute_input":"2023-05-29T16:59:05.027478Z","iopub.status.idle":"2023-05-29T16:59:05.044471Z","shell.execute_reply.started":"2023-05-29T16:59:05.027449Z","shell.execute_reply":"2023-05-29T16:59:05.043474Z"},"jupyter":{"source_hidden":true}}},{"cell_type":"markdown","source":"gpt_trans_loop(112, 1301, 75, 75, 3.5)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T16:59:10.177000Z","iopub.execute_input":"2023-05-29T16:59:10.178095Z","iopub.status.idle":"2023-05-29T16:59:19.687878Z","shell.execute_reply.started":"2023-05-29T16:59:10.178056Z","shell.execute_reply":"2023-05-29T16:59:19.686840Z"}}},{"cell_type":"markdown","source":"gpt_trans(112, 1301, 75, 3.5)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T16:45:35.919047Z","iopub.execute_input":"2023-05-29T16:45:35.919446Z","iopub.status.idle":"2023-05-29T16:45:50.715194Z","shell.execute_reply.started":"2023-05-29T16:45:35.919418Z","shell.execute_reply":"2023-05-29T16:45:50.713932Z"}}},{"cell_type":"markdown","source":"get_q(112, 1301, 1)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:43:20.739888Z","iopub.execute_input":"2023-05-30T18:43:20.740281Z","iopub.status.idle":"2023-05-30T18:43:20.751402Z","shell.execute_reply.started":"2023-05-30T18:43:20.740253Z","shell.execute_reply":"2023-05-30T18:43:20.749993Z"}}},{"cell_type":"markdown","source":"## Code for checking translation","metadata":{}},{"cell_type":"markdown","source":"def compare(year, code, question):\n    # Define the file paths\n    file_path1 = \"/kaggle/input/taiwan-112/\" + str(year) + \"_\" + str(code) + \".csv\"\n    file_path2 = \"/kaggle/input/taiwan-112-translated/\" + str(year) + \"_\" + str(code) + \"_model3.5.csv\"\n    file_path3 = \"/kaggle/input/taiwan-112-translated/\" + str(year) + \"_\" + str(code) + \"_model4.csv\"\n\n    try:\n        df1 = pd.read_csv(file_path1, encoding='utf-8')\n    except UnicodeDecodeError:\n        print(\"Could not read the first file with the tried encoding.\")\n        return\n\n    try:\n        df2 = pd.read_csv(file_path2, encoding='utf-8')\n        df3 = pd.read_csv(file_path3, encoding='utf-8')\n    except UnicodeDecodeError:\n        print(\"Could not read the second or third file with the tried encoding.\")\n        return\n\n    # Find and print the desired row from the first file\n    row1 = df1[df1[df1.columns[2]] == question]\n    if len(row1) > 0:\n        print(\"ORIGINAL QUESTION: \" + row1.iloc[0, 3])\n    else:\n        print(\"No rows match the given question in the first file.\")\n    print()\n\n    # Find and print the desired row from the second file\n    row2 = df2[(df2[df2.columns[0]] == year) & (df2[df2.columns[1]] == code) & (df2[df2.columns[2]] == question)]\n    if len(row2) > 0:\n        print(\"GPT-3.5 TRANSLATION: \" + row2.iloc[0, 7])\n    else:\n        print(\"No rows match the given year, code, and question in the second file.\")\n    print()\n\n    # Find and print the desired row from the third file\n    row3 = df3[(df3[df3.columns[0]] == year) & (df3[df3.columns[1]] == code) & (df3[df3.columns[2]] == question)]\n    if len(row3) > 0:\n        print(\"GPT-4   TRANSLATION: \" + row3.iloc[0, 7])\n    else:\n        print(\"No rows match the given year, code, and question in the third file.\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T17:13:07.890274Z","iopub.execute_input":"2023-05-29T17:13:07.890666Z","iopub.status.idle":"2023-05-29T17:13:07.902366Z","shell.execute_reply.started":"2023-05-29T17:13:07.890638Z","shell.execute_reply":"2023-05-29T17:13:07.901120Z"}}},{"cell_type":"markdown","source":"# Test the function\ncompare(112, 2302, 4)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T17:14:11.283073Z","iopub.execute_input":"2023-05-29T17:14:11.283486Z","iopub.status.idle":"2023-05-29T17:14:11.312937Z","shell.execute_reply.started":"2023-05-29T17:14:11.283454Z","shell.execute_reply":"2023-05-29T17:14:11.311890Z"}}},{"cell_type":"markdown","source":"## Answer Key for 112","metadata":{}},{"cell_type":"markdown","source":"ans_112_1301 = [\n    'A', 'D', 'A', 'C', 'A', 'D', 'D', 'A', 'A', 'C', \n    'C', 'D', 'C', 'D', 'B', 'B', 'C', 'A', 'C', 'D',\n    'A', 'D', 'D', 'C', 'D', 'C', 'D', 'C', 'D', 'C', \n    'C', 'A', 'D', 'A', 'C', 'A', 'B', 'C', 'A', 'B',\n    'B', 'C', 'A', 'B', 'B', 'B', 'D', 'B', 'A', 'A', \n    'A', 'D', 'D', 'A', 'C', 'A', 'D', 'A', 'B', 'D',\n    'A', 'D', 'A', 'D', 'C', ['B', 'D'], 'D', 'A', 'D', 'D',\n    'A', 'C', 'C', 'B', 'D', 'D', 'D', 'B', 'B', 'D',\n    'D', 'C', 'B', 'B', 'A', 'D', 'C', 'B', 'C', 'D', \n    'B', 'A', 'C', 'D', 'D', 'B', 'C', 'B', 'C', 'B'\n]\n\nans_112_1302 = [\n    'D', 'B', 'C', 'D', 'D', 'C', 'C', 'B', 'D', 'C',\n    'A', 'C', 'A', 'D', 'C', 'A', 'C', 'A', 'C', 'B',\n    'A', 'B', 'C', 'B', 'D', 'A', 'B', 'C', 'B', 'D',\n    'A', 'C', 'A', 'C', 'D', 'D', 'D', 'B', 'C', 'C',\n    'B', 'D', 'C', 'A', 'B', 'C', ['A', 'B', 'C', 'D'], 'C', 'C', 'B',\n    'D', 'B', 'D', 'B', 'A', 'B', 'C', 'A', 'A', 'A', \n    'A', 'D', 'C', 'D', 'C', 'A', 'D', 'D', 'B', 'B', \n    'B', 'C', 'A', 'A', 'D', 'D', 'D', 'A', 'C', 'D'\n]\n\nans_112_2301 = [\n    'B', 'B', 'D', 'C', 'D', 'B', 'C', 'B', 'B', 'C',\n    'B', 'A', 'B', 'C', 'B', 'A', 'D', 'A', 'B', 'C',\n    'B', 'C', 'A', 'D', 'A', 'D', 'D', 'B', 'C', 'A',\n    'B', 'A', 'D', 'C', 'C', ['A', 'B', 'C', 'D'], 'B', 'B', 'C', 'B',\n    'A', 'D', 'B', 'D', 'A', 'B', 'B', 'D', 'C', 'A',\n    'B', 'A', 'C', 'B', 'B', 'C', 'B', 'B', 'A', 'C',\n    'A', 'C', 'C', 'A', 'D', 'D', 'B', 'B', ['A', 'B', 'C', 'D'], 'D',\n    'A', 'A', 'A', 'A', 'A', 'A', 'D', 'C', 'D', 'D',\n    'A', 'D', 'C', 'B', 'B', 'D', 'A', 'C', 'A', 'C',\n    'A', 'D', 'C', 'D', 'B', 'B', 'D', 'D', 'D', 'B'\n]\n\nans_112_2302 = [\n    'B', 'A', 'C', 'D', 'B', 'B', 'C', 'D', 'B', 'B',\n    'A', 'D', 'A', 'A', 'C', 'B', 'D', 'B', 'C', 'B',\n    'B', 'C', 'D', 'D', 'B', 'A', 'C', 'C', 'B', 'B',\n    'A', 'A', 'D', 'B', 'D', 'B', 'D', 'C', 'B', 'B',\n    'B', 'B', 'C', 'A', 'B', 'C', 'A', 'D', 'D', 'B',\n    'C', 'D', 'A', 'B', 'D', 'B', 'B', 'C', 'C', 'C',\n    'C', 'C', 'D', 'C', 'C', 'A', 'C', 'C', 'A', 'B',\n    'D', 'D', 'C', 'C', 'B', 'A', 'C', 'C', 'C', 'D'\n]\n\nans_112_3302 = [\n    'B', 'B', 'C', 'C', 'B', 'A', 'D', 'A', 'D', 'B',\n    'D', 'D', 'D', 'D', 'A', 'B', 'C', 'B', 'B', 'A',\n    'C', 'D', 'A', 'C', 'B', 'B', 'D', 'B', 'A', 'D',\n    'D', 'D', 'A', 'B', 'D', 'D', 'C', 'B', 'A', 'B',\n    'D', 'D', 'B', 'B', 'B', 'A', 'C', 'A', 'D', 'C',\n    'C', 'C', 'D', 'C', 'B', 'D', 'B', 'A', 'C', 'A',\n    'A', 'A', 'C', 'C', 'A', 'A', 'A', 'A', 'B', 'C',\n    'D', 'C', 'B', 'B', 'B', 'D', 'B', 'A', 'B', 'C'\n]\n\nans_112_4302 = [\n    'A', 'D', 'B', 'B', 'C', 'C', 'D', 'C', 'D', 'A', \n    'A', 'A', 'B', 'D', 'D', 'B', 'B', 'C', 'A', 'A', \n    'B', 'A', 'B', 'D', 'A', 'C', 'D', 'B', 'B', 'C', \n    'A', 'D', 'A', 'A', 'B', 'D', 'A', 'C', 'A', 'A', \n    'A', 'D', 'C', 'D', 'C', 'D', 'C', 'A', 'C', 'A', \n    'A', 'B', 'D', 'B', 'D', 'B', 'D', 'A', 'A', 'B', \n    'A', 'C', 'A', 'B', 'D', 'C', 'D', 'C', 'C', 'D', \n    'C', 'A', 'C', 'B', 'D', 'D', 'C', 'C', 'D', 'A'\n]\n\ndef get_ans(year, code, question):\n    answers = globals()[f\"ans_{year}_{code}\"]\n    ans = answers[question - 1] \n    if isinstance(ans, list):\n        if set(ans) == set(['A', 'B', 'C', 'D']):\n            return '送分'\n        else:\n            return ' or '.join(ans)\n    else:\n        return ans","metadata":{"execution":{"iopub.status.busy":"2023-05-27T17:31:43.263666Z","iopub.execute_input":"2023-05-27T17:31:43.264095Z","iopub.status.idle":"2023-05-27T17:31:43.298285Z","shell.execute_reply.started":"2023-05-27T17:31:43.264064Z","shell.execute_reply":"2023-05-27T17:31:43.297125Z"}}},{"cell_type":"markdown","source":"get_ans(112,1302, 48)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T17:32:31.414309Z","iopub.execute_input":"2023-05-27T17:32:31.414741Z","iopub.status.idle":"2023-05-27T17:32:31.422269Z","shell.execute_reply.started":"2023-05-27T17:32:31.414706Z","shell.execute_reply":"2023-05-27T17:32:31.420968Z"}}},{"cell_type":"markdown","source":"## Code for Q bank parsing","metadata":{}},{"cell_type":"markdown","source":"import re\nimport pandas as pd\n\ntext = input()\n\nyear_pattern = re.compile(r\"(\\d+)+年\")\nyear = year_pattern.search(text)\nif year:\n    year = year.group(1)\n\nsubject_code_pattern = re.compile(r\"代 號:(\\d+)\")\nsubject_code_match = subject_code_pattern.search(text)\nif subject_code_match:\n    subject_code = subject_code_match.group(1)\n\nquestion_list = []\nnext_question_number = 1\nfor part in re.split(r\"(\\d+)\\.\", text):\n    try:\n        current_number = int(part.strip())\n        if current_number == next_question_number:\n            question_list.append({\n                'Year': year,\n                'Subject Code': subject_code,\n                'Question Number': current_number,\n                'Question and Options': '',\n            })\n            next_question_number += 1\n        else:\n            if question_list:\n                question_list[-1]['Question and Options'] += part.strip()\n    except ValueError:\n        if question_list: \n            question_list[-1]['Question and Options'] += part.strip()\n\ndf = pd.DataFrame(question_list)\ndisplay(df)\n\nfile_name = f\"{year}_{subject_code}.csv\"\ndf.to_csv(file_name, index=False)","metadata":{"_kg_hide-input":true}}]}